{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72caff4",
   "metadata": {},
   "source": [
    "### Deep Learning for autonomous vehicules\n",
    "# Milestone 1: Yolov5 + Mediapipe hand\n",
    "Group : Aziz Belkhiria Nadia Hadjmbarek Ilyas Ben Adada Vincent Naayem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822abf2a",
   "metadata": {},
   "source": [
    "### Define detection mode : 'detection' (on the robot) , 'testing' (on the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a052c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'testing'\n",
    "first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350240bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3129cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f38a4e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Aziz/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-4-27 torch 1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Model definition for yolov5\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "model.classes =[0,] #Only detects humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbd98c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition for mediapipe\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=4, min_detection_confidence=0.6)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "model_mp= load_model('hand-gesture-recognition-code/mp_hand_gesture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bb3d4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centers (results,frame):\n",
    "    i = 0\n",
    "    centers = np.zeros(np.shape(results)[0])\n",
    "    for (xmin,ymin,xmax,ymax,conf,classe) in results:\n",
    "        frame = cv2.rectangle(frame,(int(xmin),int(ymin)),(int(xmax),int(ymax)),(0,0,255),2) \n",
    "        centers[i] = (xmin + xmax)/2\n",
    "        i = i+1\n",
    "    return centers, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "41111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest (centers, landmark):\n",
    "    min_dist = 1000\n",
    "    idx = 0\n",
    "    for i in range(np.size(centers)): \n",
    "        dist = abs(landmark - centers[i])\n",
    "        if (dist < min_dist):\n",
    "            idx = i\n",
    "            min_dist = dist\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a7a7898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    img = frame.copy()\n",
    "    \n",
    "    #find persons with yolo\n",
    "    results = model(img) \n",
    "    #find bounding box centers for persons\n",
    "    centers,frame = find_centers(results.xyxy[0],frame)\n",
    "\n",
    "    # BGR 2 RGB\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    y , x, c = img.shape\n",
    "\n",
    "    # Set flag\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Mediapipe detection\n",
    "    results_mp = hands.process(img)\n",
    "\n",
    "    # Set flag to true\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # RGB 2 BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Find Mediapipe landmarks \n",
    "    if results_mp.multi_hand_landmarks:\n",
    "        for handslms in results_mp.multi_hand_landmarks: # Loop through the hands\n",
    "            landmarks = []\n",
    "            for lm in handslms.landmark:\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "                \n",
    "            # Predict hand gesture    \n",
    "            prediction = model_mp.predict([landmarks]) \n",
    "            classID = np.argmax(prediction)\n",
    "               \n",
    "            if classID == 1:    #If peace sign is detected\n",
    "                mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "                # Find the closest person by comparing the position on x axis\n",
    "                idx = closest(centers,landmarks[0][0])\n",
    "                # Draw a bounding box around the person of interest\n",
    "                xminbox = int(results.xyxy[0][idx][0])\n",
    "                yminbox = int(results.xyxy[0][idx][1])\n",
    "                xmaxbox = int(results.xyxy[0][idx][2])\n",
    "                ymaxbox = int(results.xyxy[0][idx][3])\n",
    "                frame = cv2.rectangle(frame,(xminbox,yminbox),(xmaxbox,ymaxbox),(0,255,0),2)\n",
    "                \n",
    "                if (mode == 'detection') and (first == True):\n",
    "                    ROI = img[yminbox:ymaxbox,xminbox:xmaxbox,:]\n",
    "                    first = False  \n",
    "                \n",
    "    cv2.imshow('Detection', frame)\n",
    "    \n",
    "    if first == False and mode == 'detection':\n",
    "        first = True\n",
    "        break\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3dcffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI =  cv2.cvtColor(ROI, cv2.COLOR_BGR2RGB)\n",
    "ROI = plt.imshow(ROI)\n",
    "plt.savefig('detected.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3aa66d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('detected.png')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
